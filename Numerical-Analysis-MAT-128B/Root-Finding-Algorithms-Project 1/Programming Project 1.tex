\documentclass[final,12pt,reqno]{amsart}

\setlength{\textwidth }{7.50 in}
\setlength{\textheight}{9.25 in}
\setlength{\oddsidemargin }{0.00 in}
\setlength{\evensidemargin}{0.00 in}
\setlength{\oddsidemargin }{0.00 in}
\setlength{\evensidemargin}{0.00 in}
\setlength{\hoffset}{-0.50 in}
\setlength{\voffset}{-0.50 in}
\setlength{\headsep}{12 pt}
\setlength{\headheight}{40.53336 pt}
\setlength{\topmargin }{00 pt}
\setlength{\footskip}{0.50 in}
\setlength{\parskip}{12 pt}
\setlength{\parindent}{00 pt}
\setlength{\fboxsep}{10 pt}

\usepackage{graphicx}
\DeclareGraphicsExtensions{.png}
\graphicspath{{C:/Users/Christopher/Desktop/"MAT 128B"/"Project 1"}}

\usepackage{enumerate}
\usepackage{multicol}
\usepackage{bm}
\usepackage{color}

\usepackage[bookmarks=true]{hyperref}
\usepackage{bookmark}

\usepackage{verbatim}
\usepackage{slashbox}

\font\myfiverm=cmr5 scaled 500

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{}
\fancyhead[LO]{}
\fancyhead[LE]{}
\fancyhead[CO]{\textbf{Programming Project 1}}
\fancyhead[CE]{\textbf{Programming Project 1}}
\fancyhead[RO]{\textbf{Christopher Wong\\
                        999234204\\
												MAT 128B\\
                        2/1/2017}}
\fancyhead[RE]{\textbf{Christopher Wong\\
                        999234204\\
												MAT 128A\\
                        2/1/2017}}
												
\newcommand\abs[1]{\left|#1\right|}
\renewcommand{\arraystretch}{1.2}

%%%%%%%%%%%%%%%%%%%%%%% START OF DOCUMENT %%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\thispagestyle{fancy}

\pdfbookmark[1]{Problem 1}{problem1}
\textbf{Problem 1}

\pdfbookmark[2]{Part a}{problem1a}
\textbf{(a)}

\[
f(x) = x + lnx.
\]

Using $a_0$ = 0.5 and $b_0$ = 1 for REGFAL, $x_0$ = 0.5 and $x_1$ = 1 as starting values for SECANT, and $x_0$ = 1 as initial guess for NEWTON, we obtain:

\begin{center}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		Program & Iterations & Evaluations of $f$ & Evaluations of $f'$ & $x_k$ & $f(x_k)$\\
		\hline
		REGFAL & 7 & 9 & NA & 5.671432904097840e-01 & 2.220446049250313e-16\\
		\hline
		SECANT & 6 & 8 & NA & 5.671432904097838e-01 & -1.110223024625157e-16\\
		\hline
		NEWTON & 5 & 6 & 5 & 5.671432904097838e-01 & -1.110223024625157e-16\\
		\hline
	\end{tabular}
\end{center}

We see that Newton's method converges the fastest as expected because it has quadratic convergence. After comes the secant method with its order of convergence between linear and quadratic and lastly the regula falsi method whose convergence can be the same as the bisection method (linear convergence) or even slower. In this problem, all three methods converged to the zero of the function.

\pdfbookmark[2]{Part b}{problem1b}
\textbf{(b)}

\[
f(x) = 1 - 10x + 0.01e^x.
\]

Using $a_0$ = 5 and $b_0$ = 20 for REGFAL, $x_0$ = 5 and $x_1$ = 20 as starting values for SECANT, and $x_0$ = 20 as initial guess for NEWTON, we obtain:

\begin{center}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		Program & Iterations & Evaluations of $f$ & Evaluations of $f'$ & $x_k$ & $f(x_k)$\\
		\hline
		REGFAL & 154708 & 154710 & NA & 9.105602120447736e+00 & -4.649251650334918e-09\\
		\hline
		SECANT & 7 & 9 & NA & 1.011063943496088e-01 & 6.071532165918825e-17\\
		\hline
		NEWTON & 17 & 18 & 17 & 9.105602120505811e+00 & -5.684341886080802e-14\\
		\hline
	\end{tabular}
\end{center}

We notice something strange about our zero(s) from our programs! There are two zeros to this function and it just so happens that Newton's method converged slower than the secant method to the larger zero of the function. The secant method converged to the smaller zero of the function the fastest of the programs and the regula falsi method converged EXTREMELY slow to the larger zero of the function. The regular falsi method program also terminated differently than the other two because its $\abs{f(x_k)} > FTOL = 10^{-15}$ so it stopped as a result of the $x_k$ values having a difference of less than XTOL = $10^{-15}$ for each additional iteration after 154,708.

\newpage

\pdfbookmark[2]{Part c}{problem1c}
\textbf{(c)}

\[
f(x) = x^2 - 2xe^{-x} + e^{-2x}.
\]

Running NEWTON with initial guess $x_0$ = 1:

\begin{center}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		Program & Iterations & Evaluations of $f$ & Evaluations of $f'$ & $x_k$ & $f(x_k)$\\
		\hline
		NEWTON & 25 & 26 & 25 & 5.671433018515338e-01 & 3.885780586188048e-16\\
		\hline
	\end{tabular}
\end{center}

We notice that while Newton's method converged to a zero, it did so slowly. Newton's method has the property of quadratic convergence so we expect a very rapid convergence to the zero especially since we started close to it ($x_0$ = 1).

\pdfbookmark[2]{Part d}{problem1d}
\textbf{(d)}

\[
f(x) = \text{arctan}x - \frac{2x}{1+x^2}.
\]

Using $a_0$ = 1 and $b_0$ = 2 for REGFAL, $x_0$ = 1 and $x_1$ = 2 as starting values for SECANT, and $x_0$ = 1 as initial guess for NEWTON, we obtain:

\begin{center}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		Program & Iterations & Evaluations of $f$ & Evaluations of $f'$ & $x_k$ & $f(x_k)$\\
		\hline
		REGFAL & 6 & 8 & NA & 1.391745200270735e+00 & 2.220446049250313e-16\\
		\hline
		SECANT & 5 & 7 & NA & 1.391745200270735e+00 & 0\\
		\hline
		NEWTON & 4 & 5 & 4 & 1.391745200270735e+00 & 2.220446049250313e-16\\
		\hline
	\end{tabular}
\end{center}

All three programs found the same positive zero of the function. We see that Newton's method converges the fastest as expected because it has quadratic convergence. After comes the secant method with its order of convergence between linear and quadratic and lastly the regula falsi method whose convergence can be the same as the bisection method (linear convergence) or even slower.

\pdfbookmark[2]{Part e}{problem1e}
\textbf{(e)}

\[
f(x) = \text{arctan}x.
\]

Using the zero found in part (d) as initial guess for NEWTON we obtain an error stating that the derivative of our zero estimate $f'(x_k)$ = 0 after 46 iterations. So we conclude that Newton's method diverges for this function and initial guess. The estimates before the error are given below:

\begin{center}
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		Iterations & Evaluations of $f$ & Evaluations of $f'$ & $x_k$ & $f(x_k)$\\
		\hline
		46 & 46 & 45 & -3.359067911765232e+181 & -1.570796326794897e+00\\
		\hline
	\end{tabular}
\end{center}

\newpage

\pdfbookmark[2]{Part f}{problem1f}
\textbf{(f)}

\[
f(x) = \text{arctan}x.
\]

Using NEWTON with the specified initial guesses $x_0$ gives:

\begin{center}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		$x_0$ & Iterations & Evaluations of $f$ & Evaluations of $f'$ & $x_k$ & $f(x_k)$\\
		\hline
		1 & 5 & 6 & 5 & 0 & 0\\
		\hline
		5 & ERROR AT: 9 & 9 & 8 & 1.309270537307534e+214 & 1.570796326794897e+00\\
		\hline
		50 & ERROR AT: 8 & 8 & 7 & -4.625029064936211e+241 & -1.570796326794897e+00\\
		\hline
		100 & ERROR AT: 8 & 8 & 7 & -3.570421266828119e+280 & -1.570796326794897e+00\\
		\hline
	\end{tabular}
\end{center}

We notice that starting at $x_0$ = 1 is the only initial guess that makes Newton's method converge to a zero. The remaining initial guesses $x_0$ = 5, 50, 100 make Newton's method diverge after 8 or 9 iterations when the derivative at the estimate $x_k$, $f'(x_k) < 10^{-16} = 0$.

\newpage

\pdfbookmark[1]{Problem 2}{problem2}
\textbf{Problem 2}

\pdfbookmark[2]{Part a}{problem2a}
\textbf{(a)}

\[
f(x) = \text{arctan}x - \frac{2x}{1+x^2}.
\]

Using NEWTON with the specified initial guesses $x_0$ gives:

	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		$x_0$ & Iterations & Evaluations of $f$ & Evaluations of $f'$ & $x_k$ & $f(x_k)$\\
		\hline
		-40 & ERROR AT: 7 & 7 & 6 & -2.633572261133008e+83 & -1.570796326794897e+00\\
		\hline
		-30 & ERROR AT: 8 & 8 & 7 & 3.536231819534596e+149 & 1.570796326794897e+00\\
		\hline
		-5 & 5 & 6 & 5 & -1.391745200270735e+00 & -2.220446049250313e-16\\
		\hline
		10 & ERROR AT: 8 & 8 & 7 & -1.379680173705174e+78 & -1.570796326794897e+00\\
		\hline
		20 & ERROR AT: 8 & 8 & 7 & -6.696780523894109e+124 & -1.570796326794897e+00\\
		\hline
	\end{tabular}

Using NEWTON$\_$DAMPING with the specified initial guesses $x_0$ gives:

\begin{center}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		$x_0$ & Iterations & Evaluations of $f$ & Evaluations of $f'$ & $x_k$ & $f(x_k)$\\
		\hline
		-40 & 67 & 68 & 67 & -1.391745200270737e+00 & -1.221245327087672e-15\\
		\hline
		-30 & 63 & 64 & 63 & -1.391745200270737e+00 & -1.221245327087672e-15\\
		\hline
		-5 & 50 & 51 & 50 & -1.391745200270737e+00 & -1.221245327087672e-15\\
		\hline
		10 & 53 & 54 & 53 & 1.391745200270737e+00 & 1.221245327087672e-15\\
		\hline
		20 & 58 & 59 & 58 & 1.391745200270737e+00 & 1.221245327087672e-15\\
		\hline
	\end{tabular}
\end{center}

In terms of convergence, we find that when Newton's method without damping only converged if our initial guess $x_0$ was close enough to the zero of the function and did so quadratically as expected. For all other initial values, Newton's method without damping diverged. On the other hand, Newton's method with damping converged for all of our initial guesses albeit a lot slower than the expected quadratic convergence of Newton's method. We also notice that when our initial guess converged to the closest zero of the function (i.e. negative initial guesses will converge to the negative zero of f and positive initial guesses will converge to the positive zero of f).

\newpage

\pdfbookmark[2]{Part b}{problem2b}
\textbf{(b)}

\[
f(x) = \frac{1-x}{1+x}, \quad x\>-1.
\]

Using NEWTON with the specified initial guesses $x_0$ gives:

\begin{center}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		$x_0$ & Iterations & Evaluations of $f$ & Evaluations of $f'$ & $x_k$ & $f(x_k)$\\
		\hline
		2 & 6 & 7 & 6 & 1 & 0\\
		\hline
		5 & ERROR AT: 10 & 10 & 9 & -2.681561585988510e+154 & -1\\
		\hline
		10 & ERROR AT: 9 & 9 & 8 & -3.337594124014076e+167 & -1\\
		\hline
		100 & ERROR AT: 8 & 8 & 7 & -1.623661373928269e+217 & -1\\
		\hline
	\end{tabular}
\end{center}

Using NEWTON$\_$DAMPING with the specified initial guesses $x_0$ gives:

\begin{center}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		$x_0$ & Iterations & Evaluations of $f$ & Evaluations of $f'$ & $x_k$ & $f(x_k)$\\
		\hline
		2 & 1 & 2 & 1 & 2 & -3.333333333333333e-01\\
		\hline
		5 & 1 & 2 & 1 & 5 & -6.666666666666666e-01\\
		\hline
		10 & 1 & 2 & 1 & 10 & -8.181818181818182e-01\\
		\hline
		100 & 1 & 2 & 1 & 100 & -9.801980198019802e-01\\
		\hline
	\end{tabular}
\end{center}

We find that Newton's method without damping converged if our initial guess $x_0$ (in this case $x_0$ = 2) was close enough to the zero of the function and did so quadratically as expected. For all other initial values, Newton's method without damping diverged. For this function, Newton's method with damping does not change the estimate of the zero from the initial guess for these values because the next estimate was too close to our initial guess. That is, $\abs{x_k - x_{k-1}} \leq (1 + \abs{x_k})\text{XTOL}$. This is a result of the small derivative at these points forcing the damping factor $\lambda_k$ to be very small. Because of this, we find
\[
	x_{k+1} = x_k - \lambda_k\frac{f(x_k}{f'(x_k)} \approx x_k.
\]
So our program for Newton's method with damping immediately terminates and returns the initial estimate.

\newpage

\pdfbookmark[1]{Code Appendix}{code_appendix}
\textbf{Code Appendix}

\pdfbookmark[2]{REGFAL}{regfal}

\textbf{REGFAL.m}
\verbatiminput{C:/Users/Christopher/Desktop/"MAT 128B"/"Project 1"/"REGFAL.m"}

\newpage

\pdfbookmark[2]{SECANT}{secant}

\textbf{SECANT.m}
\verbatiminput{C:/Users/Christopher/Desktop/"MAT 128B"/"Project 1"/"SECANT.m"}

\newpage

\pdfbookmark[2]{NEWTON}{newton}

\textbf{NEWTON.m}
\verbatiminput{C:/Users/Christopher/Desktop/"MAT 128B"/"Project 1"/"NEWTON.m"}

\newpage

\pdfbookmark[2]{NEWTON_DAMPING}{newton_damping}

\textbf{NEWTON$\_$DAMPING.m}
\verbatiminput{C:/Users/Christopher/Desktop/"MAT 128B"/"Project 1"/"NEWTON_DAMPING.m"}

\end{document} 